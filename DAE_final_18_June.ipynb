{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unemployment rate in SA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from scipy import stats\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "from statsmodels.formula.api import rlm\n",
    "import statsmodels.api as sm\n",
    "from sklearn.utils import resample\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Government has published a 25-year ‘review’ focusing on the progress made by South Africa since democracy in 1994 in areas such as unemployment. Citing data from Stats SA, the review shows that 8.9 million people were employed in 1994, with an unemployment rate of 20%. However, it should be noted that the unemployment rate at that time did not include the Bantustans and the black majority. In 1994, there were 41 million South Africans, therefore the employed represented 21% of the population. By the end of 2018, the number of people employed had almost doubled to 16.5 million people, representing 28.5% of the population. Despite this, and as a consequence of an increasing population growth which surpassed the economic growth, the unemployment rate has increased to 27.1%.\n",
    "This unemployment rate has continued to climb in 2019, reaching 29.1% in the third quarter – its highest rate in over 16 years. The country’s unemployment rate last reached 28% in 2003. With that being said, the purpose of this notebook is to explore variables that could potetially have a relationship with our response variables, i.e unemployment rate.\n",
    "\n",
    "https://businesstech.co.za/news/business/353051/south-africa-unemployment-1994-vs-2019/\n",
    "\n",
    "We will first start of by exploring which varibles have a relationship with the unemployment rate then after use those variables to answer the following questions:\n",
    "\n",
    "### Research questions\n",
    "\n",
    "1. How does the government cash flow affect unemployment rate? <br>\n",
    "<t>- Unemployment adversely affects the disposable income of families, erodes purchasing power, diminishes employee morale, and reduces an economy's output, with that being said, I would like to investigate the effect of cash flow has on unemployment rate. <br>\n",
    "2. Do investment returns affect the employment rate?<br>\n",
    "decribe\n",
    "\n",
    "3. How does the government assets affect unemployment rate?<br>\n",
    "decribe\n",
    "\n",
    "4. How does the government debt affect unemployment rate?<br>\n",
    "\n",
    "\n",
    "\n",
    "### Overview of Methodology\n",
    "\n",
    "To start, the data is cleaned to remove any deformities. Emperical analysis is then done, on the now cleaned data, to identify the key variables that were used in the modelling phase. Using these key variables an initial model was formed and evaluated. Following this initial model, exploratory modelling was done to further select features that were used in the final model. Hypothesis testing was used to evaluate the quality of the final model as well as provide additional information to assist in answering the research question.\n",
    "\n",
    "### Section of Contents:\n",
    "- 1. Data Description\n",
    "- 2. Data Collection\n",
    "- 3. Reading in Data\n",
    "- 4. Data Preparation\n",
    "- 5. Empirical Analysis\n",
    "- 6. Model Fitting\n",
    "-      6.* Hypothesis Testing\n",
    "-      6.* Interpretation of Results\n",
    "- 7.Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Description\n",
    "\n",
    "The data was accessed\n",
    "from the South African Reserve Bank (SARB). The SARB mostly collects and reports its\n",
    "own data, however, a few features are sourced from Statistics South Africa. Therefore, this\n",
    "data is reliable and the data generating process is transparent and accessible on the SARB\n",
    "website: this refered to as the Special Data Dissemination Standard (SDDS). The data has\n",
    "coverage of a number of key economic sectors in South Africa: real, fiscal, financial, and\n",
    "external sector as well as population data.\n",
    "It is also work noting that economic data is typically available in constant prices or current prices. Constant prices are prices as at a given date, therefore, the value today is not\n",
    "affected by economic changes that would not make a non-financial difference to it. For example inflation causes prices to change, not because anything has changed about the goods\n",
    "or services but because time has passed. Current prices are those that incorporate these\n",
    "financial changes such as inflation. For this research we elected to use constant prices to\n",
    "avoid inflation being a confounding variable across across our data. Inflation (CPI) is itself\n",
    "a feature that was used for this research. <br>\n",
    "\n",
    "\n",
    "* Collection method:  **draft**\n",
    "* Date collected: April 16, 2015\n",
    "* Date Downloaded: April 07, 2021\n",
    "* Data size: 1432 rows, 147 columns\n",
    "\n",
    "\n",
    "In order to measure the quality of the data we need to make sure it conforms with the 5 aspects of data quality namely Validity, Accuracy, Completeness, Consistency, Uniformity\n",
    "\n",
    "- Validity: The data conforms to a standard format but contains a few nulls which could impact our outcomes for the specific questions that we mentioned above. (See 4.Data Preparation)\n",
    "- Accuracy: The data was collected from actual South African Reserve Bank Website and is therefore accurate and conforms to the real world. (See 2. Data Collection)\n",
    "- Completeness: There are null entries almost in all the columns, this can be seen in section (4.Data Preparation). There are no duplicate entries present.\n",
    "- Consistency: Upon looking at the data, data in fields and columns respectively appear to be in logical agreement. (See 3. Reading Data)\n",
    "- Uniformity: By looking at the dataset, we are able to confirm that the same units are used across a given field. For example, all money in the ‘Net cash-flow from operating activities’ column are given in Rands and all the rates entries in the ‘unemployment rate’ column are given in percentages.\n",
    "\n",
    "Ability to Answer Question\n",
    "\n",
    "The dataset contains the government economic and financial statistics data in South Africa from South African Reserve Bank which compiles high-quality economic and financial statistics based on international best practice for use by policymakers, financial market participants and the general public. This reduces biasness and allows us to have the ability to generalize the answer to the proposed questions to the country level perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### South African Reserve Bank Cleaned Economic Data\n",
    "\n",
    "Data prepared for modelling from the South African Reserve Bank\n",
    "\n",
    "This data can used for both regression and classification research questions i.e. forecast the unemployment rate.\n",
    "\n",
    "The original data was sourced from https://www.resbank.co.za/en/home/what-we-do/statistics/releases/economic-and-financial-data-for-south-africa\n",
    "\n",
    "\n",
    "### 1.1 The full feature set\n",
    "*These feature were accessed from the South African Reserve Bank.*\n",
    "\n",
    "*There are **147 features in total**, these cover a significant portfion of the South African economy*\n",
    "\n",
    "**The data from 1922-01-01 to 2020-01-01** if it used for unemployment forecasting, deleting redudant observations is helpful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set_sarb = pd.read_csv('sarb_features_data.csv').set_index('Date') # reading in data\n",
    "target = pd.read_csv('sarb_target_data.csv').set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Final consumption expenditure by general government</th>\n",
       "      <th>Consolidated general government: Revenue</th>\n",
       "      <th>Foreign liabilities: Total portfolio investment</th>\n",
       "      <th>Foreign liabilities: Portfolio investment: Equity securities</th>\n",
       "      <th>Domestic output: All groups</th>\n",
       "      <th>Final consumption expenditure by households: Total</th>\n",
       "      <th>Gross fixed capital formation</th>\n",
       "      <th>SDDS - Financial derivative liabilities</th>\n",
       "      <th>Foreign liabilities: Portfolio investment: Debt securities</th>\n",
       "      <th>Change in inventories</th>\n",
       "      <th>...</th>\n",
       "      <th>Remuneration per worker in non-agricultural: Total</th>\n",
       "      <th>Consolidated general government: Non-financial assets - Net</th>\n",
       "      <th>Consolidated general government: Cash surplus / deficit</th>\n",
       "      <th>CPI Headline</th>\n",
       "      <th>Gross domestic expenditure</th>\n",
       "      <th>Net cash-flow from operating activities</th>\n",
       "      <th>Non-agricultural employment: Total</th>\n",
       "      <th>Consolidated general government: Expense</th>\n",
       "      <th>Residual item</th>\n",
       "      <th>unemployment rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1922-01-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1922-02-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1922-03-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1922-04-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1922-05-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 148 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Final consumption expenditure by general government   \\\n",
       "Date                                                               \n",
       "1922-01-01                                                NaN      \n",
       "1922-02-01                                                NaN      \n",
       "1922-03-01                                                NaN      \n",
       "1922-04-01                                                NaN      \n",
       "1922-05-01                                                NaN      \n",
       "\n",
       "            Consolidated general government: Revenue  \\\n",
       "Date                                                   \n",
       "1922-01-01                                       NaN   \n",
       "1922-02-01                                       NaN   \n",
       "1922-03-01                                       NaN   \n",
       "1922-04-01                                       NaN   \n",
       "1922-05-01                                       NaN   \n",
       "\n",
       "            Foreign liabilities: Total portfolio investment   \\\n",
       "Date                                                           \n",
       "1922-01-01                                               NaN   \n",
       "1922-02-01                                               NaN   \n",
       "1922-03-01                                               NaN   \n",
       "1922-04-01                                               NaN   \n",
       "1922-05-01                                               NaN   \n",
       "\n",
       "            Foreign liabilities: Portfolio investment: Equity securities  \\\n",
       "Date                                                                       \n",
       "1922-01-01                                                NaN              \n",
       "1922-02-01                                                NaN              \n",
       "1922-03-01                                                NaN              \n",
       "1922-04-01                                                NaN              \n",
       "1922-05-01                                                NaN              \n",
       "\n",
       "            Domestic output: All groups   \\\n",
       "Date                                       \n",
       "1922-01-01                           NaN   \n",
       "1922-02-01                           NaN   \n",
       "1922-03-01                           NaN   \n",
       "1922-04-01                           NaN   \n",
       "1922-05-01                           NaN   \n",
       "\n",
       "            Final consumption expenditure by households: Total   \\\n",
       "Date                                                              \n",
       "1922-01-01                                                NaN     \n",
       "1922-02-01                                                NaN     \n",
       "1922-03-01                                                NaN     \n",
       "1922-04-01                                                NaN     \n",
       "1922-05-01                                                NaN     \n",
       "\n",
       "            Gross fixed capital formation   \\\n",
       "Date                                         \n",
       "1922-01-01                             NaN   \n",
       "1922-02-01                             NaN   \n",
       "1922-03-01                             NaN   \n",
       "1922-04-01                             NaN   \n",
       "1922-05-01                             NaN   \n",
       "\n",
       "            SDDS - Financial derivative liabilities  \\\n",
       "Date                                                  \n",
       "1922-01-01                                      NaN   \n",
       "1922-02-01                                      NaN   \n",
       "1922-03-01                                      NaN   \n",
       "1922-04-01                                      NaN   \n",
       "1922-05-01                                      NaN   \n",
       "\n",
       "            Foreign liabilities: Portfolio investment: Debt securities  \\\n",
       "Date                                                                     \n",
       "1922-01-01                                                NaN            \n",
       "1922-02-01                                                NaN            \n",
       "1922-03-01                                                NaN            \n",
       "1922-04-01                                                NaN            \n",
       "1922-05-01                                                NaN            \n",
       "\n",
       "            Change in inventories   ...  \\\n",
       "Date                                ...   \n",
       "1922-01-01                     NaN  ...   \n",
       "1922-02-01                     NaN  ...   \n",
       "1922-03-01                     NaN  ...   \n",
       "1922-04-01                     NaN  ...   \n",
       "1922-05-01                     NaN  ...   \n",
       "\n",
       "            Remuneration per worker in non-agricultural: Total   \\\n",
       "Date                                                              \n",
       "1922-01-01                                                NaN     \n",
       "1922-02-01                                                NaN     \n",
       "1922-03-01                                                NaN     \n",
       "1922-04-01                                                NaN     \n",
       "1922-05-01                                                NaN     \n",
       "\n",
       "            Consolidated general government: Non-financial assets - Net  \\\n",
       "Date                                                                      \n",
       "1922-01-01                                                NaN             \n",
       "1922-02-01                                                NaN             \n",
       "1922-03-01                                                NaN             \n",
       "1922-04-01                                                NaN             \n",
       "1922-05-01                                                NaN             \n",
       "\n",
       "            Consolidated general government: Cash surplus / deficit  \\\n",
       "Date                                                                  \n",
       "1922-01-01                                                NaN         \n",
       "1922-02-01                                                NaN         \n",
       "1922-03-01                                                NaN         \n",
       "1922-04-01                                                NaN         \n",
       "1922-05-01                                                NaN         \n",
       "\n",
       "            CPI Headline   Gross domestic expenditure   \\\n",
       "Date                                                     \n",
       "1922-01-01            0.6                          NaN   \n",
       "1922-02-01            0.6                          NaN   \n",
       "1922-03-01            0.6                          NaN   \n",
       "1922-04-01            0.6                          NaN   \n",
       "1922-05-01            0.6                          NaN   \n",
       "\n",
       "            Net cash-flow from operating activities   \\\n",
       "Date                                                   \n",
       "1922-01-01                                       NaN   \n",
       "1922-02-01                                       NaN   \n",
       "1922-03-01                                       NaN   \n",
       "1922-04-01                                       NaN   \n",
       "1922-05-01                                       NaN   \n",
       "\n",
       "            Non-agricultural employment: Total   \\\n",
       "Date                                              \n",
       "1922-01-01                                  NaN   \n",
       "1922-02-01                                  NaN   \n",
       "1922-03-01                                  NaN   \n",
       "1922-04-01                                  NaN   \n",
       "1922-05-01                                  NaN   \n",
       "\n",
       "            Consolidated general government: Expense  Residual item   \\\n",
       "Date                                                                   \n",
       "1922-01-01                                       NaN             NaN   \n",
       "1922-02-01                                       NaN             NaN   \n",
       "1922-03-01                                       NaN             NaN   \n",
       "1922-04-01                                       NaN             NaN   \n",
       "1922-05-01                                       NaN             NaN   \n",
       "\n",
       "            unemployment rate  \n",
       "Date                           \n",
       "1922-01-01                NaN  \n",
       "1922-02-01                NaN  \n",
       "1922-03-01                NaN  \n",
       "1922-04-01                NaN  \n",
       "1922-05-01                NaN  \n",
       "\n",
       "[5 rows x 148 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_set_sarb.head() # displaying first 5 rows for exploratory varibales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unemployment rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1922-01-01</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1922-02-01</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1922-03-01</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1922-04-01</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1922-05-01</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            unemployment rate\n",
       "Date                         \n",
       "1922-01-01                NaN\n",
       "1922-02-01                NaN\n",
       "1922-03-01                NaN\n",
       "1922-04-01                NaN\n",
       "1922-05-01                NaN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.head() # displaying first 5 rows for target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Preparation\n",
    "\n",
    "In this section we are going to clean raw dataset by using different techniques(Joins, removing duplicates, cleaning variables, changing types and aggregations) to remove incomplete, unreliable, or faulty entries before it’s analyzed and leveraged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sense check\n",
    "Checking if the distribution of the data before cleaning and after cleaning \n",
    "\n",
    "#### Sense check of raw data\n",
    "Exploratory features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_set_sarb.plot.hist(bins=12, alpha=0.5,legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.plot.hist(bins=12, alpha=0.5,legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determining null values in the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_set_sarb.isnull().sum() # Determining null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set_sarb.isnull().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains null values which can prevent us from working with the data. We use data imputation to fill in cells with null values. We utilise the forward fill and backward fill method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Imputation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data imputation strategy is foward fill i.e last know value imputation\n",
    "# Economic data usually does not change that much from month to month.\n",
    "x_values_ffill = feature_set_sarb.fillna(method='bfill')\n",
    "x_values_ffill = feature_set_sarb.fillna(method='ffill')\n",
    "y_values_ffill = target.fillna(method='bfill')\n",
    "y_values_ffill = target.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all data points before unemployment rate data is available. Unemployment rate is my target variable.\n",
    "valid_start = y_values_ffill.first_valid_index()\n",
    "y_values_ffill = y_values_ffill[valid_start : ]\n",
    "x_values_ffill = x_values_ffill[valid_start : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We fill with NA here to avoid any features that might be NA i.e. insurance\n",
    "x_values_ffill = x_values_ffill.fillna(feature_set_sarb.mean())\n",
    "y_values_ffill = y_values_ffill.fillna(target.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values_ffill.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After filling the empty cells, the data is now complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Chaning types\n",
    "\n",
    "Here we check the datatypes of our features and change them to appropriate types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values_ffill.dtypes # checking types for exploratory fetures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_values_ffill.dtypes # checking types for target feture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the features have correct types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Slicing data\n",
    "\n",
    "Here we remove all the features that were recorded and remain with those in the period of **1994 -2020** since **we are investigating unemployement rate in SA in 1994 - 2020**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values_ffill = x_values_ffill.loc['1994':'2021'] # getting exploratory features from 1994-2020\n",
    "print(x_values_ffill.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_values_ffill = y_values_ffill.loc['1994':'2021'] # getting target features from 1994-2020\n",
    "print(y_values_ffill)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sense check for cleaned data\n",
    "Exploratory variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values_ffill.plot.hist(bins=12, alpha=0.5,legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Targe variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_values_ffill.plot.hist(bins=12, alpha=0.5,legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the distributions of the raw vs cleaned data are not the same, this shows that all outdated or incorrect information are gone leaving with the highest quality information which will improve the performance output of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Empirical Analysis\n",
    "\n",
    "In this section we will be exploring our data by examining the structure and components of our dataset. The reason behind this section is because we want to:\n",
    "\n",
    "- identify whether or not the dataset has any flaws\n",
    "- assess whether the dataset can answer the questions we're asking.\n",
    "- make a rough sketch of the response to our questions \n",
    "\n",
    "The approach we are going to take is to explore the dataset based on each member's question perspective and make a general conclusion to the main question. Our emperical analysis will be guided by our questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function\n",
    "Below are the helper functions that help with removing unnecessary characters and selecting features based on the repsective questions from the original features names. <br>\n",
    "\n",
    "Reason for removing unnecessary characters from the columns name is because of the we want to make it clean and avoid complain from pandas library when using arbitrary names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(a, b):\n",
    "    \n",
    "    \"\"\"Get features: groups features based on the repsective questions\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    a and b: The common key words to access the features from original dataset\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    df_new: a list containing all the column names required based on the key words\n",
    "           \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    df_new = list() # empty list to store features\n",
    "    x_col = x_values_ffill.columns.tolist() # converting explorator features to list\n",
    "    for i in x_col:    # look for features based on key words\n",
    "        if a != '' and a in i:\n",
    "            df_new.append(x_values_ffill[i])\n",
    "            print(i)\n",
    "        if b != '' and b in i:\n",
    "            df_new.append(x_values_ffill[i])\n",
    "            print(i)\n",
    "    return df_new # return a list with feature names\n",
    "\n",
    "\n",
    "\n",
    "def wordopt(text):\n",
    "    \n",
    "    \"\"\"Wordopt: removes ambigous characters from the column names\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    text: a string, contains column name\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    text: a new text with no ambiguous characters\n",
    "           \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    text = text.lower() \n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub(\"\\\\W\",\" \",text) \n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unemployement rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly,before we explore variables that could potentially have a relationship with unemployment rate, the unemployment rate from the year 1994-2020 is plotted. The plots differs by a period 6 years from 1994 to 2021, for the remaining years 2018-2021 it's only 4 years. The reason behind plotting in range of 5 years is to make them display all information without discarding some information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen in the plots, the unemployment rate in the years 1995-1999 increases exponentially at almost a constant rate.  The unemployment rate in the years 2000-2005 increases exponentially then steadly descreases as it reached the peak where it remained constant for some time in the year 2000 then descreases afterwards. The unemployment rate in the years 2006-2011 decreases dramtically and then steadly oscillates. This is in line with the with the findings of external source (https://businesstech.co.za/news/trending/77737/south-africa-unemployment-1994-2015/) \n",
    "<br>\n",
    "\n",
    "The unemployment rate in the years 2012-2017 oscillates and increases at the end and lastly in the years 2018-2021, it  increases exponentially at a steady rate then remain constant for some time. This is in line with the findings of external source (https://www.southafricanmi.com/south-africas-unemployment.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Unemplyoment rate from 1994-1999\")\n",
    "plt.ylabel(\"unemployment rate in %\")\n",
    "plt.xlabel(\"Years\")\n",
    "plt.plot(x_1994_2020.index, y_values_ffill[\"unemployment rate\"].loc['1994':'1999'], label = \"line 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "x_2000_2005 = x_values_ffill.loc['2000':'2005']\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Unemplyoment rate from 2000-2005\")\n",
    "plt.ylabel(\"unemployment rate in %\")\n",
    "plt.xlabel(\"Years\")\n",
    "plt.plot(x_2000_2005.index, y_values_ffill[\"unemployment rate\"].loc['2000':'2005'], label = \"line 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "\n",
    "\n",
    "x_2006_2011 = x_values_ffill.loc['2006':'2011']\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Unemplyoment rate from 2006-2011\")\n",
    "plt.ylabel(\" unemployment rate in %\")\n",
    "plt.xlabel(\"Years\")\n",
    "plt.plot(x_2006_2011.index, y_values_ffill[\"unemployment rate\"].loc['2006':'2011'], label = \"line 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "\n",
    "x_2012_2017 = x_values_ffill.loc['2012':'2017']\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Unemplyoment rate form 2012-2017\")\n",
    "plt.ylabel(\"unemployment rate in %\")\n",
    "plt.xlabel(\"Years\")\n",
    "plt.plot(x_2012_2017.index, y_values_ffill[\"unemployment rate\"].loc['2012':'2017'], label = \"line 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "\n",
    "x_2018_2020 = x_values_ffill.loc['2018':'2021']\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Unemplyoment rate form 2018-2021\")\n",
    "plt.ylabel(\"unemployment rate in %\")\n",
    "plt.xlabel(\"Years\")\n",
    "plt.plot(x_2018_2020.index, y_values_ffill[\"unemployment rate\"].loc['2018':'2021'], label = \"line 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Exploring variables that could potentially have a relationship with unemployment rate based on different perspectives\n",
    "\n",
    "Below we explore the dataset based on different member's perspective question. We are going to explore each features for each perspective and conclude whether the main objectives of this section (mentioned above) are fulfilled for both our repsective perspectives and main question. Reason behind using this approach is to avoid being arbitary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 QUESTION 1 FEATURES: ASSETS\n",
    "\n",
    "Here we are going to explore features of the first perspective of the main question: **How does the government assets affect unemployment rate?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assets2 = get_features('assets','') # getting features\n",
    "df_assets2 = pd.DataFrame(data = df_assets2) # creating a new dataset based on these new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Staticstics\n",
    "Mean is usually greater than the median. These observations indicate that there are outliers in the data set and before the final modeling is performed outliers must be taken care of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_assets2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_inv = get_inv_features('evalua', 'nvest') \n",
    "# DF_INV = pd.DataFrame(data = df_inv)\n",
    "\n",
    "\n",
    "\n",
    "# X_ASSETS = df_assets2.transpose()\n",
    "# df2 = df2.rename(columns={'unemployment rate':'unemployment_rate'})\n",
    "# y_inv = df2['unemployment_rate']\n",
    "# X_INV.columns.tolist()\n",
    "\n",
    "df_assets2 = df_assets2.T\n",
    "\n",
    "df_assets2.append(x_values_ffill[\"unemployment rate\"])\n",
    "# df_assets2\n",
    "\n",
    "for col in df_assets2.columns:\n",
    "    new_w = wordopt(col)\n",
    "    df_assets2.rename(columns= {col: new_w.replace(' ', '')}, inplace = True)\n",
    "\n",
    "print(df_assets2.columns)\n",
    "\n",
    "del df_assets2[\"otherreserveassets\"]\n",
    "corr_df = df_assets2.corr(method='pearson')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "\n",
    "mask=np.zeros_like(corr_df)\n",
    "mask[np.triu_indices_from(mask)]=True\n",
    "sns.heatmap(corr_df,cmap='RdYlGn_r',vmax=1.0,vmin=-1.0,mask=mask,linewidths=2.5,ax=ax)\n",
    "plt.yticks(rotation=0)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each square shows the correlation between the variables on each axis. Values closer to zero means there is no linear trend between the two variables. The close to 1 the correlation is the more positively correlated they are; that is as one increases so does the other and the closer to 1 the stronger this relationship is. A correlation closer to -1 is similar, but instead of both increasing one variable will decrease as the other increases. The diagonals are all 1/dark green because those squares are correlating each variable to itself (so it's a perfect correlation). For the rest the larger the number and darker the color the higher the correlation between the two variables. The plot is also symmetrical about the diagonal since the same two variables are being paired together in those squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax = sns.heatmap(df_assets2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 QUESTION 2 FEATURES: INVESTMENT RETURNS\n",
    "\n",
    "Here we are going to explore features of the second perspective of the main question: **Do investment returns affect the employment rate?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inv = get_features('evalua', 'nvest') \n",
    "DF_INV = pd.DataFrame(data = df_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_INV= DF_INV.transpose()\n",
    "df2 = x_values_ffill.rename(columns={'unemployment rate':'unemployment_rate'})\n",
    "y_inv = df2['unemployment_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in X_INV.columns:\n",
    "    new_w = wordopt(col)\n",
    "    X_INV.rename(columns= {col: new_w.replace(' ', '')}, inplace = True)\n",
    "X_INV.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary statistics\n",
    "It can be seen also in this plot that the main is usually greater than the median. These observations indicate that there are outliers in these variables and before the final modeling is performed outliers must be taken care of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_INV.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df2 = X_INV.corr(method='pearson')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "\n",
    "mask=np.zeros_like(corr_df2)\n",
    "mask[np.triu_indices_from(mask)]=True\n",
    "sns.heatmap(corr_df2,cmap='RdYlGn_r',vmax=1.0,vmin=-1.0,mask=mask,linewidths=2.5,ax=ax)\n",
    "plt.yticks(rotation=0)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax = sns.heatmap(X_INV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 QUESTION 3 FEATURES: CASHFLOW\n",
    "\n",
    "Here we are going to explore features of the third perspective of the main question: **How does the government cash flow affect unemployment rate?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cash = get_features('cash', '') # get appropriate feature names\n",
    "DF = pd.DataFrame(data = df_cash) # create a new dataset which only contains appropriate varibales that need to be reseache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF= DF.transpose()\n",
    "some_df = x_values_ffill.rename(columns={'unemployment rate':'unemployment_rate'})\n",
    "y_inv = some_df['unemployment_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in DF.columns:\n",
    "    new_w = wordopt(col)\n",
    "    DF.rename(columns= {col: new_w.replace(' ', '')}, inplace = True)\n",
    "DF.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary statistics\n",
    "The median can been seen to be greater than the mean in almost all the features. These observations indicate that there are less outliers in these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df3 = DF.corr(method='pearson')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "\n",
    "mask=np.zeros_like(corr_df3)\n",
    "mask[np.triu_indices_from(mask)]=True\n",
    "sns.heatmap(corr_df3,cmap='RdYlGn_r',vmax=1.0,vmin=-1.0,mask=mask,linewidths=5,ax=ax)\n",
    "plt.yticks(rotation=0)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax = sns.heatmap(DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 QUESTION 4 FEATURES: DEBT AND OUTSTANDING BALANCES\n",
    "\n",
    "\n",
    "Here we are going to explore features of the fourth perspective to the main question: **How does the government debt affect unemployment rate?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_1 = get_features('debt', 'outstanding')\n",
    "df_new_1 = pd.DataFrame(df_new_1)\n",
    "df_new_1 = df_new_1.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary statistics\n",
    "The mean is slightly greater than the median in this case. These observations indicate that there are slighly more outliers in these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df3 = DF.corr(method='pearson')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "\n",
    "mask=np.zeros_like(corr_df3)\n",
    "mask[np.triu_indices_from(mask)]=True\n",
    "sns.heatmap(corr_df3,cmap='RdYlGn_r',vmax=1.0,vmin=-1.0,mask=mask,linewidths=5,ax=ax)\n",
    "plt.yticks(rotation=0)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax = sns.heatmap(df_new_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_new_1.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model fitting: Unemployment rate in SA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does different economic factors affect Unemployment rate ?\n",
    "\n",
    "The purpose of this notebook is to explore variables that could potetially have a relationship with our response variables, i.e unemployment rate.\n",
    "<br>\n",
    "<br>\n",
    "We will first start of by exploring which varibles have a relationship with the unemployment rate then after use those variables to answer the following questions:\n",
    "<br>\n",
    "<br>\n",
    "**Questions:**<br>\n",
    "1. How does the government cash flow affect unemployment rate?\n",
    "2. Do investment returns affect the employment rate?\n",
    "3. How does the government assets affect unemployment rate?\n",
    "4. How does the government debt affect unemployment rate?\n",
    "<br>\n",
    "<br>\n",
    "We will explore this relationship using the regression slope test that has a regression line of the format:\n",
    "<br>\n",
    "$$\n",
    "Y=\\beta_{i} X \\ for \\ i = 0,1,2...m\n",
    "$$\n",
    "Where $X$ are the selected variables and $\\beta_{i}$ are the respective coefficients.Thus, the **hypothesis** is as follows:\n",
    "$$\n",
    "\\begin{array}{l}{\\mathrm{H}_{\\mathrm{0}} : \\beta_{0}=\\beta_{1}=...=\\beta_{m}=0} \\\\ {\\mathrm{H}_{\\mathrm{1}} : \\beta_{i} \\neq 0 } \\ for \\ at \\ least\\ one \\ i\\end{array}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "We will be using the $F$-test to simultaneously check the significance of a number of regression coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  6.1 How does the government assets affect unemployment rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions are common throughout the entire 4 models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward selection typically begins with only an intercept. One tests the various variables that may be relevant, and the ‘best’ variable where “best” is determined by some pre-determined criteria—is added to the model.<br>\n",
    "As the model continues to improve (per that same criteria) we continue the process, adding in one variable at a time and testing at each step. Once the model no longer improves with adding more variables, the process stops. <br>\n",
    "\n",
    "The Linear model function below is designed by Forward selection using the R2 score for measurement. **(Fill in here)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit(X,y):\n",
    "    \n",
    "    \"\"\"Linear model designed by forward selection.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X: pandas DataFrame with all possible predictors and response\n",
    "\n",
    "    y: decimal values, containing target variable\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    score_list: a list containing the R2 coefficients of determining statistical \n",
    "    measure of how well the regression predictions approximate the real data points\n",
    "           \n",
    "    optimum_no_features: Variables with the optimum features\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #no of features\n",
    "    feature_list=np.arange(1, len(X.columns))            \n",
    "    high_score=0\n",
    "    #Variable to store the optimum features\n",
    "    optimum_no_featurcashes=0           \n",
    "    score_list =[]\n",
    "    for n in range(len(feature_list)):\n",
    "        model = LinearRegression()\n",
    "        rfe = RFE(model,feature_list[n])\n",
    "        X_train_rfe = rfe.fit_transform(X,y)\n",
    "        model.fit(X_train_rfe,y)\n",
    "        # R2 score for measurement\n",
    "        score = r2_score(y, model.predict(X_train_rfe))\n",
    "        score_list.append(score)\n",
    "        \n",
    "        if n % 10 == 0:\n",
    "            print(n,score)\n",
    "            \n",
    "        if(score>high_score):\n",
    "            high_score = score  \n",
    "            optimum_no_features = feature_list[n]\n",
    "    return score_list, optimum_no_features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function for R squared coefficient line graph plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \"\"\".\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    values: list,R squared coefficient values\n",
    "    title : string, title of the R squared coefficient plot \n",
    "    x_label: string, x-axis label\n",
    "    y_label : string, y-axis label\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "def plt_plot(values,title, x_label, y_label):\n",
    "    \n",
    "    plt.plot(values)\n",
    "    plt.title(title)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.xlabel(x_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \"\"\"Linear model designed by forward selection.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    optimum_no_features: pandas DataFrame with all possible predictors and response\n",
    "\n",
    "    X_train: decimal values, containing target variable\n",
    "    y_train:\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    score_list: a list containing the R2 coefficients of determining statistical \n",
    "    measure of how well the regression predictions approximate the real data points\n",
    "           \n",
    "    optimum_no_features: Variables with the optimum features\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def final_model(optimum_no_features, X_train, y_train,cols):\n",
    "    model = LinearRegression()\n",
    "    #Initializing RFE model\n",
    "    rfe = RFE(model, optimum_no_features)             \n",
    "    #Transforming data using RFE\n",
    "    x_fitted = rfe.fit_transform(X_train,y_train)  \n",
    "    #Fitting the data to model\n",
    "    model.fit(x_fitted,y_train)              \n",
    "    temp = pd.Series(rfe.support_,index = cols)\n",
    "    selected_features = temp[temp==True].index\n",
    "    return selected_features, model, rfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_p_values(x_test, y_test, predicted, params):\n",
    "    \"\"\"\n",
    "    Calculates the p value, based on https://stackoverflow.com/a/42677750/9260653\n",
    "    \"\"\"\n",
    "    newX = np.append(np.ones((len(x_test),1)), x_test, axis=1)\n",
    "    MSE = (sum((y_test.values-predicted)**2))/(len(newX)-len(newX[0]))\n",
    "\n",
    "    var_b = MSE*(np.linalg.pinv(np.dot(newX.T,newX)).diagonal())\n",
    "    sd_b = np.sqrt(var_b)[:-1]\n",
    "    ts_b = params/ sd_b\n",
    "\n",
    "    p_values =[2*(1-stats.t.cdf(np.abs(i),(len(newX)-1))) for i in ts_b]\n",
    "\n",
    "    sd_b = np.round(sd_b,3)\n",
    "    ts_b = np.round(ts_b,3)\n",
    "    p_values = np.round(p_values,3)\n",
    "    params = np.round(params,4)\n",
    "\n",
    "    myDF3 = pd.DataFrame()\n",
    "    myDF3[\"Coefficients\"],myDF3[\"Standard Errors\"],myDF3[\"t values\"],myDF3[\"P-Values\"] = [params,sd_b,ts_b,p_values]\n",
    "    print(myDF3.iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From exploratory analysis it appears most of the assets are positively correlated with unemployment rate, however, there is also strong correlation between some assets which may lead to problem of colinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here I am separating features from the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= df_assets2\n",
    "df2 = y_values_ffill.rename(columns={'unemployment rate':'unemployment_rate'})\n",
    "\n",
    "y = df2['unemployment_rate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier on, we saw that some of our fetaures we strongly correlated that could cause issue of multicollinearity, to resolve that, I shall optimize my feature space by removing features with correlation more than 0.8 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_features = set()\n",
    "correlation_matrix = X.corr()\n",
    "\n",
    "for i in range(len(correlation_matrix .columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            correlated_features.add(colname)\n",
    "len(correlated_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(correlated_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop(labels=correlated_features, axis=1, inplace=True)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having obtained the optimized features, below we select them and in our new dataset and plot the correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = X.corr(method='pearson')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "\n",
    "\n",
    "mask=np.zeros_like(corr_df)\n",
    "mask[np.triu_indices_from(mask)]=True\n",
    "sns.heatmap(corr_df,cmap='RdYlGn_r',vmax=1.0,vmin=-1.0,mask=mask,linewidths=2.5,ax=ax)\n",
    "plt.yticks(rotation=0)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning the Data into Training(70%) and Testing(30%) sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following function applies step wise linear regression fit to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_list, optimum_no_features = model_fit(X_train,y_train)\n",
    "print(optimum_no_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_plot(scores_list, 'Score vs Number of varibales selected','Number of variables', 'Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final model which utilizes optimum number of features, which were returned by the above function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_assets = list(X.columns)\n",
    "\n",
    "selected_features, model, rfe = final_model(optimum_no_features, X_train, y_train,cols_assets)\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our stepwise linear regression we have ended with 5 features only 1 redundant one was removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predicted = model.predict(rfe.transform((X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Stepwise Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals =y_train.astype(float)- (y_train_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.residplot(y_train.astype(float),y_train_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula_features = ' + '.join(list(selected_features))\n",
    "formula = 'unemployment_rate~'+ formula_features\n",
    "formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add y back to df\n",
    "\n",
    "X_y_new_train = pd.concat([X_train,pd.DataFrame(y_train)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_y_target = pd.concat([X_train, pd.DataFrame(y_train)], axis=1)\n",
    "X_train_y_target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = sm.ols(formula=formula,data = X_train_y_target).fit()\n",
    "import statsmodels.graphics as smgraphics\n",
    "\n",
    "axss = smgraphics.regressionplots.plot_fit(results, 2)\n",
    "_ = smgraphics.regressionplots.plot_fit(results, 1)\n",
    "_ = smgraphics.regressionplots.plot_fit(results, 3)\n",
    "_ = smgraphics.regressionplots.plot_fit(results, 4)\n",
    "_ = smgraphics.regressionplots.plot_fit(results, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_ols_pred=results.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_train - Y_ols_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val = results.fittedvalues.copy()\n",
    "fig, ax = plt.subplots(figsize=(6,2.5))\n",
    "\n",
    "\n",
    "_ = ax.scatter(pred_val, residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(6,2.5))\n",
    "_ = ax1.scatter(X_train.iloc[:,2:3], residuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error distribution of residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(residuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QQ plots of sample quantiles vs theoretical ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "_ = sm.qqplot(residuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier removal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = results.outlier_test()\n",
    "outliers = ((x[i],y[i]) for i,t in enumerate(test) if t[2] < 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers =(i for i,t in enumerate(test.iloc[:,2]) if t < 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index of Outliers\n",
    "outliers_list = list(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "X_train = pd.DataFrame(np.delete(X_train.values, outliers_list,0))\n",
    "y_train = pd.DataFrame(np.delete(y_train.values, outliers_list,0))\n",
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "final_model = rlm(formula, data=X_train_y_target,\n",
    "                      M=sm.robust.norms.HuberT()).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_predicted = final_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = np.array(final_model.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.r2_score(y_test, final_model_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_p_values(X_test, y_test, final_model_predicted, params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test for $H_{0}$ by using the following statistic:\n",
    "$$\n",
    "F_{0} = \\frac{MS_R}{MS_E}\n",
    "$$\n",
    "where $MS_R$ is the regression mean square and $MS_E$ is the error mean square.\n",
    "<br>\n",
    "The null hypothesis, $H_{0}$, is rejected if the calculated statistic. $f_{0}, is usch that$\n",
    "$$\n",
    "F_{0} > f\n",
    "$$\n",
    "I will now test my hypothesist using $F$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fstat_test(final_model):\n",
    "    A = np.identity(len(final_model.params))\n",
    "    A = A[1:,:]\n",
    "    fvalue = final_model.f_test(A).fvalue\n",
    "    print(\"The fvalue is \" + str(fvalue[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fstat_test(final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degresOfFreedom = len(X_train) - (len(X_train.columns)+1)\n",
    "scipy.stats.f.ppf(q=1-0.05, dfn=2, dfd=degresOfFreedom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, $F_{0} = 110.5337$ and $f = 3.0121$.\n",
    "<br>\n",
    "<br>\n",
    "Since $F_0$ > $f$, $H_{0}$ is rejected and it is concluded that at least one $\\beta_{i}$ cofficient is significant. In other words, it is concluded that a regression model exists between the exploratory and response variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2 Unemployement rate in SA: Investigating the relationship between governments profits/losses on investments and unemployment rate in South Africa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Do investment returns affect the employment rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to removr investment features with correlation scores of over 70 percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_features = set()\n",
    "correlation_matrix = X_INV.corr()\n",
    "\n",
    "for i in range(len(correlation_matrix .columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.7:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            correlated_features.add(colname)\n",
    "len(correlated_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_INV.drop(labels=correlated_features, axis=1, inplace=True)\n",
    "X_INV.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train_inv, X_test_inv, y_train_inv, y_test_inv = train_test_split(X_INV,y_inv, test_size = 0.3, random_state = 0)\n",
    "X_train_inv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invest_model_fit(X_fit,y_fit):    \n",
    "    feature_list=np.arange(1, len(X_train_inv.columns))            \n",
    "    high_score=0\n",
    "    optimum_no_features=0           \n",
    "    score_list =[]\n",
    "    for n in range(len(feature_list)):\n",
    "        model = LinearRegression()\n",
    "        rfe = RFE(model,feature_list[n])\n",
    "        X_train_rfe = rfe.fit_transform(X_train_inv,y_train_inv)\n",
    "        model.fit(X_train_rfe,y_train_inv)\n",
    "\n",
    "        score = r2_score(y_fit, model.predict(X_train_rfe))\n",
    "        score_list.append(score)\n",
    "\n",
    "        if(score>high_score):\n",
    "            high_score = score  \n",
    "            optimum_no_features = feature_list[n]\n",
    "    return score_list, optimum_no_features \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_list_inv, optimum_no_features_inv = invest_model_fit(X_train_inv,y_train_inv)\n",
    "print(\"Number of features selected:\",optimum_no_features_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_plot(scores_list, 'Score vs Number of varibales selected','Number of variables', 'Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The r squared scores seem to be increasing for the selected investment features. The scores remain less that 0.4 probably because of the small number of features used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_inv_model(n, X_final_inv, y_final_inv):\n",
    "    cols = list(X_INV.columns)\n",
    "    model = LinearRegression()\n",
    "    #Initializing RFE model\n",
    "    rfe = RFE(model, n)             \n",
    "    #Transforming data using RFE\n",
    "    x_fitted = rfe.fit_transform(X_final_inv,y_final_inv)  \n",
    "    #Fitting the data to model\n",
    "    model.fit(x_fitted,y_final_inv)              \n",
    "    temp = pd.Series(rfe.support_,index = cols)\n",
    "    selected_features = temp[temp==True].index\n",
    "    return selected_features, model, rfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimum_no_features_inv)\n",
    "selected_inv_features, inv_model, inv_rfe = final_inv_model(optimum_no_features_inv, X_train_inv, y_train_inv)\n",
    "selected_inv_features.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_inv_predicted = inv_model.predict(inv_rfe.transform((X_train_inv)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_residuals =y_train_inv.astype(float)- (y_train_inv_predicted)\n",
    "sns.residplot(y_train_inv.astype(float),y_train_inv_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residual points are not evenly dustributed vertically. The predictions get worse as the unemployment rate increases. The residuals show heteroscedasticity pattern which happens when regression assumes that the residuals come data with constant variance. This resuslts in low confidence in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "_ = sm.qqplot(inv_residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(inv_residuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residuals are not normally distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula_inv_features = ' + '.join(list(selected_inv_features))\n",
    "formula_inv = 'unemployment_rate~'+ formula_inv_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_y_new_inv_train = pd.concat([X_train_inv,pd.DataFrame(y_train_inv)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_y_target_INV = pd.concat([X_train_inv, pd.DataFrame(y_train_inv)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "results = sm.ols(formula=formula_inv,data = X_train_y_target_INV).fit()\n",
    "import statsmodels.graphics as smgraphics\n",
    "\n",
    "axss = smgraphics.regressionplots.plot_fit(results, 1)\n",
    "_ = smgraphics.regressionplots.plot_fit(results, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Investment Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "final_inv_model = rlm(formula_inv, data=X_train_y_target_INV,\n",
    "                      M=sm.robust.norms.HuberT()).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_inv_model_predicted = final_inv_model.predict(X_test_inv)\n",
    "inv_params = np.array(final_inv_model.params)\n",
    "print(inv_params.shape)\n",
    "print(X_test_inv.shape)\n",
    "metrics.r2_score(y_test_inv, final_inv_model_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_p_values_inv(x_test, y_test, predicted, params):\n",
    "    \"\"\"\n",
    "    Calculates the p value, based on https://stackoverflow.com/a/42677750/9260653\n",
    "    \"\"\"\n",
    "    newX = np.append(np.ones((len(x_test),1)), x_test, axis=1)\n",
    "    MSE = (sum((y_test.values-predicted)**2))/(len(newX)-len(newX[0]))\n",
    "\n",
    "    var_b = MSE*(np.linalg.pinv(np.dot(newX.T,newX)).diagonal())\n",
    "    sd_b = np.sqrt(var_b)[:-1]\n",
    "    print(params.shape)\n",
    "    ts_b = params/ sd_b\n",
    "    \n",
    "    p_values =[2*(1-stats.t.cdf(np.abs(i),(len(newX)-1))) for i in ts_b]\n",
    "\n",
    "    sd_b = np.round(sd_b,3)\n",
    "    ts_b = np.round(ts_b,3)\n",
    "    p_values = np.round(p_values,4)\n",
    "    params = np.round(params,4)\n",
    "\n",
    "    myDF3 = pd.DataFrame()\n",
    "    myDF3[\"Coefficients\"],myDF3[\"Standard Errors\"],myDF3[\"t values\"],myDF3[\"P-Values\"] = [params,sd_b,ts_b,p_values]\n",
    "    print(myDF3.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inv_params.shape\n",
    "print_p_values_inv(X_test_inv, y_test_inv, final_inv_model_predicted, inv_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fstat_test(final_model):\n",
    "    A = np.identity(len(final_model.params))\n",
    "    A = A[1:,:]\n",
    "    fvalue = final_model.f_test(A).fvalue\n",
    "    print(\"The f value is \" + str(fvalue[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fstat_test(final_inv_model)\n",
    "degresOfFreedom = len(X_train_inv) - (len(X_train_inv.columns)+1)\n",
    "scipy.stats.f.ppf(q=1-0.05, dfn=2, dfd=degresOfFreedom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a p-value 0f less than 0.01 and f value = 101.95 greater than f = 3.012 showing high significance, we can reject the hypothesis that investment has an effect on unemployment rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3 Unemployement rate in SA: Exploring the multi-variables of cash flow in South Africa that affects the rate of unemployement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the government cash flow affect unemployment rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_cash = DF.transpose() # transpose the data to be in a correct form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emperical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section I will be analysing the features that are going to be used to model the subsdiary question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = X_cash.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplots(figsize=(25,25))\n",
    "ax = sns.heatmap(correlation, xticklabels = correlation.columns, yticklabels = correlation.columns, annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(X_cash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Collinearity\n",
    "Below I omitthe offending variables using the dimensionality reduction. This done to avoid collinearity from reducing the precision of the estimated coefficients, which weakens the statistical power of your regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshhold = 0.5 # the threshhold that is used to select features\n",
    "correlated_features_cash = set() # a set which stores features that are available after dealing with collinearity\n",
    "correlation_matrix_cash = X_cash.corr() # getting th correlation of the features\n",
    "\n",
    "for i in range(len(correlation_matrix_cash.columns)): # loop through the features\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix_cash.iloc[i, j]) > threshhold: # detecting all features that have correlation less greater than 0.5\n",
    "            colname = correlation_matrix_cash.columns[i] # selecting features with less correlation\n",
    "            correlated_features_cash.add(colname) # creating new dataset with no high correlated variables\n",
    "len(correlated_features_cash) # the remaining features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing spaces from the varibales (Data wrangling maybe???)\n",
    "This is done to make it easier work with columns name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in X_cash.columns: #selecting the features \n",
    "    new_w = wordopt(col) #removing all unnecesary characters\n",
    "    X_cash.rename(columns= {col: new_w.replace(' ', '')}, inplace = True) # removing spaces from columns names\n",
    "\n",
    "Y_cash = df2['unemployment_rate'] # removing space from target variblles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splitting\n",
    "\n",
    "The predefined ratio used for data splitting is 70/30 reason being that I want to improve the accuracy of the evaluation of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cash, X_test_cash, y_train_cash, y_test_cash = train_test_split(X_cash,Y_cash, test_size = 0.3, random_state = 0) # splitting datset into 70/30 ratio using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling - Stepwise regression\n",
    "\n",
    "In this section I employ procedures to search through the model space to select a model. In other words,it is an iterative construction procedure of a regression model that involves the selection of independent variables to be used in a final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_list_cash, optimum_no_features_cash = model_fit(X_train_cash,y_train_cash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt_plot(scores_list_cash, 'Score vs Number of varibales selected','Number of variables', 'Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward selection feature selection\n",
    "This is how the following methods behaves, at first it begins with no variables in the model, tests each variable as it is added to the model, then keeps those that are deemed most statistically significant—repeating the process until the results are optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit final model using optimum number of features from prev function\n",
    "\"\"\"final_model_cash :Linear model designed by forward selection.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train: pandas DataFrame with all the explaratory variables\n",
    "\n",
    "    y_train: pandas Series, series of response variable\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    selected_features_cash: number of selectedinput features\n",
    "    \n",
    "    model: linear regression model\n",
    "    \n",
    "    rfe: gives the ranking of all the variables, 1 being most important.\n",
    "    \"\"\"\n",
    "\n",
    "def final_model_cash(optimum_no_features, X_train, y_train):\n",
    "    cols = list(X_cash.columns)\n",
    "    model = LinearRegression()\n",
    "    #Initializing RFE model\n",
    "    rfe = RFE(model, optimum_no_features)  # RFE method takes the model to be used and the number of required features as input          \n",
    "    x_fitted = rfe.fit_transform(X_train,y_train)   #Transforming data using RFE\n",
    "    #Fitting the data to model\n",
    "    model.fit(x_fitted,y_train)              \n",
    "    temp = pd.Series(rfe.support_,index = cols)\n",
    "    selected_features = temp[temp==True].index\n",
    "    return selected_features, model, rfe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting linear model to select features for the model using forward selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_cash, model_cash, rfe_cash = final_model_cash(optimum_no_features_cash, X_train_cash, y_train_cash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using training features to predict the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predicted_cash = model_cash.predict(rfe_cash.transform((X_train_cash))) #using training features to predict the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fit Analaysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-Stepwise Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Analysis\n",
    "Here I use the resudual analysis for validating the regression model. If the dots are randomly dispersed around the horizontal axis then a linear regression model is appropriate for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_train_cash-y_train_predicted_cash # calculating the residual of the  target variable foir training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.residplot(y_train_cash,y_train_predicted_cash) #plotting the residual plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### applying normal probability plot to assess how the data (error) depart from normality visually:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating formula of the selected features for linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating formula of the selected features for linear regression model\n",
    "formula_features_cash = ' + '.join(list(selected_features_cash)) # selected features formula\n",
    "formula_cash = 'unemployment_rate~'+ formula_features_cash # target feature formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "?? not sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add y back to df\n",
    "X_y_new_train_cash = pd.concat([X_train_cash,pd.DataFrame(y_train_cash)], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_y_target_cash = pd.concat([X_train_cash, pd.DataFrame(y_train_cash)], axis=1)\n",
    "# X_train_y_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I plot the model for the features selected above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.graphics as smgraphics\n",
    "import statsmodels.formula.api as sm\n",
    "\n",
    "results_cash = sm.ols(formula=formula_cash,data = X_train_y_target_cash).fit() # fitting linear regression\n",
    "axss_cash = smgraphics.regressionplots.plot_fit(results_cash,3) # plotting regression plot for feature 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot regression plot of feature 3, **consolodated general government change in the stock cach**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_ols_pred_cash=results_cash.predict(X_train_cash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_cash = y_train_cash - Y_ols_pred_cash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(6,2.5))\n",
    "_ = ax1.scatter(X_train_cash.iloc[:,5:6], residuals_cash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "_ = sm.qqplot(residuals_cash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Detection and removal\n",
    "Here outliers are detected and deleted and used the remaining observations for fitting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cash = results.outlier_test()\n",
    "outliers_cash = ((x[i],y[i]) for i,t in enumerate(test) if t[2] < 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cash, X_test_cash, y_train_cash, y_test_cash = train_test_split(X_cash,Y_cash, test_size = 0.3, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_cash =(i for i,t in enumerate(test.iloc[:,2]) if t < 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index of Outliers\n",
    "outliers_list_cash = list(outliers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "X_train_cash = pd.DataFrame(np.delete(X_train_cash.values, outliers_list_cash,0))\n",
    "y_train_cash = pd.DataFrame(np.delete(y_train_cash.values, outliers_list_cash,0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cash, X_test_cash, y_train_cash, y_test_cash = train_test_split(X_cash,Y_cash, test_size = 0.3, random_state = 0)\n",
    "\n",
    "final_model_cash = rlm(formula_cash, data=X_train_y_target_cash,\n",
    "                      M=sm.robust.norms.HuberT()).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_predicted_cash = final_model_cash.predict(X_test_cash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_cash = np.array(final_model_cash.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.r2_score(y_test, final_model_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_p_values(X_test_cash, y_test_cash, final_model_predicted_cash, params_cash)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "he test for $H_{0}$ byusing the following statistic:\n",
    "$$\n",
    "F_{0} = \\frac{MS_R}{MS_E}\n",
    "$$\n",
    "where $MS_R$ is the regression mean square and $MS_E$ is the error mean square.\n",
    "<br>\n",
    "The null hypothesis, $H_{0}$, is rejected if the calculated statistic. $f_{0}, is usch that$\n",
    "$$\n",
    "F_{0} > f\n",
    "$$\n",
    "I will now test my hypothesist using $F$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fstat_test(final_model):\n",
    "    A = np.identity(len(final_model.params))\n",
    "    A = A[1:,:]\n",
    "    fvalue = final_model.f_test(A).fvalue\n",
    "    print(\"The fvalue is \" + str(fvalue[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fstat_test(final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degresOfFreedom = len(X_train) - (len(X_train.columns)+1)\n",
    "scipy.stats.f.ppf(q=1-0.05, dfn=2, dfd=degresOfFreedom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, $F_{0} = 110.5337$ and $f = 3.004779$.\n",
    "<br>\n",
    "<br>\n",
    "Since $F_0$ > $f$, $H_{0}$ is rejected and it is concluded that at least one $\\beta_{i}$ cofficient is significant. In other words, it is concluded that a regression model exists between the exploratory and response variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4 Unemployement rate in SA: Exploring the relationship between multiple variables that may influence the unemployment rate of South Africa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the government debt affect unemployment rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(dataset, threshold):\n",
    "    col_corr = set() # Set of all the names of deleted columns\n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if (corr_matrix.iloc[i, j] >= threshold) and (corr_matrix.columns[j] not in col_corr):\n",
    "                colname = corr_matrix.columns[i] # getting the name of column\n",
    "                col_corr.add(colname)\n",
    "                if colname in dataset.columns:\n",
    "                    del dataset[colname] # deleting the column from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing all the corrated values with a correlation value over 0.7\n",
    "DF_X = df.copy()\n",
    "correlation(DF_X,0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = list(DF_X.columns)\n",
    "keys = list('ABCDEFGH')\n",
    "\n",
    "zip1 = zip(keys, values)\n",
    "\n",
    "\n",
    "\n",
    "dictionary = dict(zip1)\n",
    "\n",
    "\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_X.columns = keys\n",
    "DF_Y.columns = ['unemployment_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "## Stepwise regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_debt, X_test_debt, y_train_debt, y_test_debt = train_test_split(DF_X,DF_Y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def final_model_debt(optimum_no_features, X_train, y_train):\n",
    "    cols = list(DF_X.columns)\n",
    "    model = LinearRegression()\n",
    "    #Initializing RFE model\n",
    "    rfe = RFE(model, optimum_no_features)             \n",
    "    #Transforming data using RFE\n",
    "    x_fitted = rfe.fit_transform(X_train,y_train)  \n",
    "    #Fitting the data to model\n",
    "    model.fit(x_fitted,y_train)              \n",
    "    temp = pd.Series(rfe.support_,index = cols)\n",
    "    selected_features = temp[temp==True].index\n",
    "    return selected_features, model, rfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model using X_train, y_train\n",
    "X_train_debt\n",
    "scores_list_debt, optimum_no_features_debt = model_fit(X_train_debt,y_train_debt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_plot(scores_list_debt, 'Score vs Number of varibales selected','Number of variables', 'Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_debt, model_debt, rfe_debt = final_model_debt(optimum_no_features_debt, X_train_debt, y_train_debt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predicted_debt = model_debt.predict(rfe_debt.transform((X_train_debt)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fit Analaysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-Stepwise Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_debt = y_train_debt-y_train_predicted_debt\n",
    "residuals_debt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.residplot(y_train_debt,y_train_predicted_debt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We can apply normal probability plot to assess how the data (error) depart from normality visually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula_features_debt = ' + '.join(list(selected_features_debt))\n",
    "formula_debt = 'unemployment_rate~'+ formula_features_debt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add y back to df\n",
    "\n",
    "X_y_new_train_debt = pd.concat([X_train_debt, y_train_debt], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_y_target_debt = pd.concat([X_train_debt, y_train_debt], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_debt = sm.ols(formula=formula_debt,data = X_train_y_target_debt).fit()\n",
    "import statsmodels.graphics as smgraphics\n",
    "\n",
    "axss = smgraphics.regressionplots.plot_fit(results_debt, 1)\n",
    "_ = smgraphics.regressionplots.plot_fit(results_debt,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_ols_pred_debt=results_debt.predict(X_train_debt)\n",
    "Y_ols_pred_debt = pd.DataFrame(Y_ols_pred_debt)\n",
    "Y_ols_pred_debt.columns = ['unemployment_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val_debt = results_debt.fittedvalues.copy()\n",
    "fig, ax = plt.subplots(figsize=(6,2.5))\n",
    "\n",
    "residuals.shape\n",
    "_ = ax.scatter(pred_val_debt, residuals_debt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(6,2.5))\n",
    "_ = ax1.scatter(X_train_debt.iloc[:,5:6], residuals_debt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot shows that the residuals are normally distributed\n",
    "sns.distplot(residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "_ = sm.qqplot(residuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Detection and removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_debt = results_debt.outlier_test()\n",
    "outliers_debt = ((x[i],y[i]) for i,t in enumerate(test_debt) if t[2] < 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_debt, X_test_debt, y_train_debt, y_test_debt = train_test_split(DF_X,DF_Y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_debt =(i for i,t in enumerate(test.iloc[:,2]) if t < 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index of Outliers\n",
    "outliers_list_debt = list(outliers_debt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers\n",
    "X_train_debt = pd.DataFrame(np.delete(X_train_debt.values, outliers_list_debt,0))\n",
    "y_train_debt = pd.DataFrame(np.delete(y_train_debt.values, outliers_list_debt,0))\n",
    "len(X_train_debt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_debt, X_test_debt, y_train_debt, y_test_debt = train_test_split(DF_X,DF_Y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "final_model_debt = rlm(formula_debt, data=X_train_y_target_debt,\n",
    "                      M=sm.robust.norms.HuberT()).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_debt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_predicted_debt = final_model_debt.predict(X_test_debt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_debt = np.array(final_model_debt.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.r2_score(y_test_debt, final_model_predicted_debt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_p_values_debt(x_test, y_test, predicted, params):\n",
    "    \"\"\"\n",
    "    Calculates the p value, based on https://stackoverflow.com/a/42677750/9260653\n",
    "    \"\"\"\n",
    "    newX = np.append(np.ones((len(x_test),1)), x_test, axis=1)\n",
    "    MSE = mean_squared_error(y_test.values,predicted.values)\n",
    "\n",
    "    var_b = MSE*(np.linalg.pinv(np.dot(newX.T,newX)).diagonal())\n",
    "    sd_b = np.sqrt(var_b)[:-1]\n",
    "    ts_b = params/ sd_b\n",
    "\n",
    "    p_values =[2*(1-stats.t.cdf(np.abs(i),(len(newX)-1))) for i in ts_b]\n",
    "\n",
    "    sd_b = np.round(sd_b,3)\n",
    "    ts_b = np.round(ts_b,3)\n",
    "    p_values = np.round(p_values,3)\n",
    "    params = np.round(params,4)\n",
    "\n",
    "    myDF3 = pd.DataFrame()\n",
    "    myDF3[\"Coefficients\"],myDF3[\"Standard Errors\"],myDF3[\"t values\"],myDF3[\"P-Values\"] = [params,sd_b,ts_b,p_values]\n",
    "    print(myDF3.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(params_debt.shape)\n",
    "print(final_model_predicted_debt.shape)\n",
    "print_p_values_debt(X_test_debt, y_test_debt, final_model_predicted_debt, params_debt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The test for $H_{0}$ byusing the following statistic:\n",
    "$$\n",
    "F_{0} = \\frac{MS_R}{MS_E}\n",
    "$$\n",
    "where $MS_R$ is the regression mean square and $MS_E$ is the error mean square.\n",
    "<br>\n",
    "The null hypothesis, $H_{0}$, is rejected if the calculated statistic. $f_{0}, is usch that$\n",
    "$$\n",
    "F_{0} > f\n",
    "$$\n",
    "I will now test my hypothesist using $F$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fstat_test(final_model):\n",
    "    A = np.identity(len(final_model.params))\n",
    "    A = A[1:,:]\n",
    "    fvalue = final_model.f_test(A).fvalue\n",
    "    print(\"The fvalue is \" + str(fvalue[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fstat_test(final_model_debt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degresOfFreedom = len(X_train) - (len(X_train_debt.columns)+1)\n",
    "scipy.stats.f.ppf(q=1-0.05, dfn=2, dfd=degresOfFreedom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, $F_{0} = 304.572$ and $f = 3.01217$.\n",
    "<br>\n",
    "<br>\n",
    "Since $F_0$ > $f$, $H_{0}$ is rejected and it is concluded that at least one $\\beta_{i}$ cofficient is significant. In other words, it is concluded that a regression model exists between the exploratory and response variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Conlcusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the null hypothesis states that there is at least one feature that is not zero, and 4 of our models rejected the null hypothesis using the F-test, therefore there is at least four varibales that affect unemployement rate in South Africa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
